## 安装

Spark 是专为大规模数据处理而设计的快速通用的计算引擎，是对 Hadoop 的补充，最大的价值就是弥补MapReduce计算性能上的不足

### SCALA 安装

`export $PATH=$PATH:$SCALA_HOME/bin`

### 单机版

`export $PATH=$PATH:$SPARK_HOME/bin`

执行 `sbin/start-master.sh`，打开 http://localhost:8080/ 即可查看

`sbin/stop-master.sh` 停止

`bin/spark-submit examples/src/main/python/pi.py`

`spark-submit --class Main xxx.jar`

`spark-submit xxx-with-dependencies.jar`

### 配置

`cp spark-env.sh.template spark-env.sh`

```shell
#export SCALA_HOME=/home/user/soft/scala
#export JAVA_HOME=/home/user/soft/jdk1.8
#export HADOOP_HOME=/home/user/soft/hadoop
#export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
#SPARK_MASTER_IP=localhost
#SPARK_LOCAL_DIRS=/home/user/soft/spark
#SPARK_DRIVER_MEMORY=1G
```


## Spark-Shell

`spark-shell --master spark://localhost:7077`
`pyspark --master spark://localhost:7077`