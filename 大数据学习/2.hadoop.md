## 安装

分布式系统基础架构，包括HDFS,MapReduce

### 单机

`export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin`

### 伪分布式

```shell
ssh-keygen -t rsa
cat ~.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 0600 ~/.ssh/authorized_keys
ssh localhost
```

`vi core-site.xml`

```xml
<configuration>
    <!-- 指定HDFS中NameNode的地址 -->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
    <property>
        <!--将hadoop的临时目录改成自定义的目录下-->
        <name>hadoop.tmp.dir</name>
        <value>/home/user/hadoop/data/tmp</value>
    </property>
</configuration>
```
`vi hdfs-site.xml`

```xml
<configuration>
    <!--副本数量-->
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <!--名称节点元数据的保存目录-->
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:/home/user/hadoop/data/tmp/dfs/name</value>
    </property>
    <!--数据节点的数据保存目录-->
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:/home/user/hadoop/data/tmp/dfs/data</value>
    </property>
</configuration>
```

`vi hadoop-env.sh`

修改JAVA_HOME `export JAVA_HOME=/usr/local/jdk1.8`

格式化namenode `hdfs namenode -format`

```shell
sbin/start-dfs.sh  # 启动
sbin/stop-dfs.sh   # 关闭
```

jps -l 出现DataNode，NameNode，SecondaryNameNode

http://localhost:50070/dfshealth.html   查看dfs状态

### 分布式

## Demo

### 单机

- mkdir input
- echo words > input.txt
- hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar wordcount input output
- cat output/part-r-00000 #查看统计

### 分布式

- 上传文件到hdfs

```shell
hdfs dfs -mkdir -p /user/hadoop/input # 在HDFS中创建input目录
hdfs dfs -put ./etc/hadoop/*.xml input  # 本地文件复制到HDFS中
hdfs dfs -ls input
```

- hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar wordcount  /usr/hadoop/input  /usr/hadoop/output
- hdfs dfs -cat /usr/hadoop/output/part-r-00000